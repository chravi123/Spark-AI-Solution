# suppress inspection "UnusedProperty" for whole file
###Basic Spark Configurations for a spark job

######################################
#Spark Properties
######################################
#Name for the spark job
spark.app.name=Spark-AI-Solution

#spark.executor.cores=4

#spark.executor.memory=4g
#
#spark.executor.instances=400
#
#spark.driver.memory=8g
#
#spark.sql.parquet.mergeSchema=false
#prev:10
spark.executor.cores=16
spark.executor.memory=9g
#newly added
spark.num.executors=12
#no of worker node * cpu in one worker node
spark.executor.instances=1
#pre 8g
spark.driver.memory=8g
spark.sql.parquet.mergeSchema=false
#no of worker node * cpu in one worker node
#prev:20
spark.default.parallelism=100

######################################
#DP Properties mano     sfsddsf
######################################

#DP app bean, this bean will launch spark job
dp.job.bean=ValidationFile

#Input location
data.input.location=XXXXXX

#Output location
dp.output.location=XXXXXXX
#dp.prev.history.days=30

#Number of files to write for output
#partisions.number=50
#partisions.number=100
#sns.alert.address=arn:aws:sns:eu-west-1:122403041129:Redshift-alerts-countdb
#sns.alert.address=arn:aws:sns:eu-west-1:122403041129:complete-spark-ingestion-latest
#sns.alert.address=arn:aws:sns:eu-west-1:794236216820:complete-spark-ingestion-beta



#Serializer
spark.serializer = org.apache.spark.serializer.KryoSerializer
final.spark.kryoserializer.buffer.max=1024mb

#Delete existing output partition
spark.hadoop.validateOutputSpecs=false
#spark.executor.memory=2g
##spark.executor.memory=4g

# new properties added
name= Livy REST API 
